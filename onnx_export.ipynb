{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc54aa09-598d-4ecc-a5e1-fb8996f8df8a",
   "metadata": {},
   "source": [
    "# Export StyleTTS2 Model\n",
    "\n",
    "See the [Colab Notebook](https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/Colab/StyleTTS2_Demo_LibriTTS.ipynb).\n",
    "\n",
    "Currently, the official implementation of StyleTTS2 does not support ONNX export. See the [discussion](https://github.com/yl4579/StyleTTS2/issues/117)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af620508-4d80-44e0-b35f-13e397556f0f",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d041c-5fc7-4956-9c6e-5b1fcb7ecc50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt-get install espeak-ng --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86874db9-6cf9-444c-a14a-a5d1b9cd66f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install SoundFile pydub pyyaml librosa nltk matplotlib phonemizer einops einops-exts tqdm typing-extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2212c3c-a31e-4ab9-a04d-9da07a53f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install build-essential libssl-dev libffi-dev python3-dev --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56baf5dc-b21a-48d6-87e1-2ec2ea90c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install libpython3.10-dev --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9346e857-69fd-4ce8-a008-406e7fd232eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/resemble-ai/monotonic_align.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ab15d3-89c0-4c61-81fe-6b5e34c4a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3562004-99f1-4660-b700-fb93ac8e12c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx onnxscript onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327661f-4f63-45b0-8cc6-73a8df92fe05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b226a9b0-aece-4742-a63f-b858302c19c8",
   "metadata": {},
   "source": [
    "## Set up demo assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25fcb6-b599-4877-b7ec-bebad5b3b04e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/yl4579/StyleTTS2-LibriTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395476f2-9583-4f51-bd9c-5f15001bca1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv StyleTTS2-LibriTTS/Models ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb7c6d-e9df-4457-aa6a-86c8d3d79ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv StyleTTS2-LibriTTS/reference_audio.zip ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819336b-2d91-403a-b996-ff07faa4542a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip reference_audio.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d4732-7805-4772-af68-e126628afd2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv reference_audio Demo/reference_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e09d1-4a9e-4cd5-bbb9-6adf4c6b3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af9564-1b3e-4ecf-b78f-e545d11f2de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75d64089-3912-4e06-b049-36f6049cb21c",
   "metadata": {
    "id": "aAGQPfgYIR23"
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3afbae-7e65-47d9-8a1a-6e760844510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e981404e-549c-4b66-8beb-dcf59de7688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8438cf-6e13-4d8b-9989-c94dd4d603d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449874b-edb5-4f53-9722-0201c69742ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac3d31-1167-434b-a6b2-c4fb1ad3a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# load packages\n",
    "import yaml\n",
    "import librosa\n",
    "from nltk.tokenize import word_tokenize\n",
    "import IPython.display as ipd\n",
    "\n",
    "from models import load_ASR_models, load_F0_models\n",
    "from text_utils import TextCleaner\n",
    "textclenaer = TextCleaner()\n",
    "\n",
    "\n",
    "def load_audio(path):\n",
    "    wave, sr = librosa.load(path, sr=24000)\n",
    "    audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "    if sr != 24000:\n",
    "        audio = librosa.resample(audio, sr, 24000)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284469ab-97c0-4c66-b088-7540aa96d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from text_utils import TextCleaner\n",
    "textclenaer = TextCleaner()\n",
    "\n",
    "\n",
    "# load phonemizer\n",
    "import phonemizer\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(language='en-us', preserve_punctuation=True,  with_stress=True)\n",
    "\n",
    "\n",
    "# start with out-of-the-box implementations in C# of word tokenizer and phenomizer\n",
    "# check how it differs with results from python implementations\n",
    "# if there are differences test performance on ONNX model\n",
    "# if results are good, we can keep the defailt C# implementations, otherwise we need to implement our own functions\n",
    "\n",
    "# rewrite text cleaner from Python function into C#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a997390-2736-4253-83db-caa3777f86b0",
   "metadata": {},
   "source": [
    "## Set up StyleTTS2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e0eb7-7d1e-4b57-b716-66a9621a65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load phonemizer\n",
    "import phonemizer\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(language='en-us', preserve_punctuation=True,  with_stress=True)\n",
    "\n",
    "config = yaml.safe_load(open(\"Models/LibriTTS/config.yml\"))\n",
    "\n",
    "# load pretrained ASR model\n",
    "ASR_config = config.get('ASR_config', False)\n",
    "ASR_path = config.get('ASR_path', False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# load pretrained F0 model\n",
    "F0_path = config.get('F0_path', False)\n",
    "pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "# load BERT model\n",
    "from Utils.PLBERT.util import load_plbert\n",
    "BERT_path = config.get('PLBERT_dir', False)\n",
    "plbert = load_plbert(BERT_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a701ed5-a283-4590-bfc2-a4d045b05c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import (\n",
    "    TextEncoder, ProsodyPredictor, StyleEncoder, StyleTransformer1d, \n",
    "    AudioDiffusionConditional, KDiffusion, LogNormalDistribution,\n",
    "    MultiPeriodDiscriminator, MultiResSpecDiscriminator, WavLMDiscriminator)\n",
    "\n",
    "def build_model(args, text_aligner, pitch_extractor, bert):\n",
    "    assert args[\"decoder\"][\"type\"] in ['istftnet', 'hifigan'], 'Decoder type unknown'\n",
    "    \n",
    "    if args[\"decoder\"][\"type\"] == \"istftnet\":\n",
    "        from Modules.istftnet import Decoder\n",
    "        decoder = Decoder(dim_in=args[\"hidden_dim\"], style_dim=args[\"style_dim\"], dim_out=args[\"n_mels\"],\n",
    "                resblock_kernel_sizes = args[\"decoder\"][\"resblock_kernel_sizes\"],\n",
    "                upsample_rates = args[\"decoder\"][\"upsample_rates\"],\n",
    "                upsample_initial_channel=args[\"decoder\"][\"upsample_initial_channel\"],\n",
    "                resblock_dilation_sizes=args[\"decoder\"][\"resblock_dilation_sizes\"],\n",
    "                upsample_kernel_sizes=args[\"decoder\"][\"upsample_kernel_sizes\"], \n",
    "                gen_istft_n_fft=args[\"decoder\"][\"gen_istft_n_fft\"], gen_istft_hop_size=args[\"decoder\"][\"gen_istft_hop_size\"]) \n",
    "    else:\n",
    "        from Modules.hifigan import Decoder\n",
    "        decoder = Decoder(dim_in=args[\"hidden_dim\"], style_dim=args[\"style_dim\"], dim_out=args[\"n_mels\"],\n",
    "                resblock_kernel_sizes = args[\"decoder\"][\"resblock_kernel_sizes\"],\n",
    "                upsample_rates = args[\"decoder\"][\"upsample_rates\"],\n",
    "                upsample_initial_channel=args[\"decoder\"][\"upsample_initial_channel\"],\n",
    "                resblock_dilation_sizes=args[\"decoder\"][\"resblock_dilation_sizes\"],\n",
    "                upsample_kernel_sizes=args[\"decoder\"][\"upsample_kernel_sizes\"])\n",
    "        \n",
    "    text_encoder = TextEncoder(channels=args[\"hidden_dim\"], kernel_size=5, depth=args[\"n_layer\"], n_symbols=args[\"n_token\"])\n",
    "    \n",
    "    predictor = ProsodyPredictor(\n",
    "        style_dim=args[\"style_dim\"], \n",
    "        d_hid=args[\"hidden_dim\"], nlayers=args[\"n_layer\"], \n",
    "        max_dur=args[\"max_dur\"], dropout=args[\"dropout\"])\n",
    "    \n",
    "    style_encoder = StyleEncoder(\n",
    "        dim_in=args[\"dim_in\"], style_dim=args[\"style_dim\"], max_conv_dim=args[\"hidden_dim\"]) # acoustic style encoder\n",
    "    predictor_encoder = StyleEncoder(\n",
    "        dim_in=args[\"dim_in\"], style_dim=args[\"style_dim\"], max_conv_dim=args[\"hidden_dim\"]) # prosodic style encoder\n",
    "        \n",
    "    # define diffusion model\n",
    "    if args[\"multispeaker\"]:\n",
    "        transformer = StyleTransformer1d(channels=args[\"style_dim\"]*2, \n",
    "                                    context_embedding_features=bert.config.hidden_size,\n",
    "                                    context_features=args[\"style_dim\"]*2, \n",
    "                                    **args[\"diffusion\"][\"transformer\"])\n",
    "    else:\n",
    "        transformer = Transformer1d(channels=args[\"style_dim\"]*2, \n",
    "                                    context_embedding_features=bert.config.hidden_size,\n",
    "                                    **args[\"diffusion\"][\"transformer\"])\n",
    "    \n",
    "    diffusion = AudioDiffusionConditional(\n",
    "        in_channels=1,\n",
    "        embedding_max_length=bert.config.max_position_embeddings,\n",
    "        embedding_features=bert.config.hidden_size,\n",
    "        embedding_mask_proba=args[\"diffusion\"][\"embedding_mask_proba\"], # Conditional dropout of batch elements,\n",
    "        channels=args[\"style_dim\"]*2,\n",
    "        context_features=args[\"style_dim\"]*2,\n",
    "    )\n",
    "    \n",
    "    diffusion.diffusion = KDiffusion(\n",
    "        net=diffusion.unet,\n",
    "        sigma_distribution=LogNormalDistribution(mean = args[\"diffusion\"][\"dist\"][\"mean\"], std = args[\"diffusion\"][\"dist\"][\"std\"]),\n",
    "        sigma_data=args[\"diffusion\"][\"dist\"][\"sigma_data\"], # a placeholder, will be changed dynamically when start training diffusion model\n",
    "        dynamic_threshold=0.0 \n",
    "    )\n",
    "    diffusion.diffusion.net = transformer\n",
    "    diffusion.unet = transformer\n",
    "\n",
    "    \n",
    "    nets = dict(\n",
    "        bert=bert,\n",
    "        bert_encoder=nn.Linear(bert.config.hidden_size, args[\"hidden_dim\"]),\n",
    "\n",
    "        predictor=predictor,\n",
    "        decoder=decoder,\n",
    "        text_encoder=text_encoder,\n",
    "\n",
    "        predictor_encoder=predictor_encoder,\n",
    "        style_encoder=style_encoder,\n",
    "        diffusion=diffusion,\n",
    "\n",
    "        text_aligner = text_aligner,\n",
    "        pitch_extractor=pitch_extractor,\n",
    "\n",
    "        mpd = MultiPeriodDiscriminator(),\n",
    "        msd = MultiResSpecDiscriminator(),\n",
    "    \n",
    "        # slm discriminator head\n",
    "        wd = WavLMDiscriminator(args[\"slm\"][\"hidden\"], args[\"slm\"][\"nlayers\"], args[\"slm\"][\"initial_channel\"]),\n",
    "       )\n",
    "    \n",
    "    return nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe3a940-c131-4e76-b380-aca9b798ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = config['model_params']\n",
    "# model_params = recursive_munch(config['model_params'])\n",
    "nets = build_model(model_params, text_aligner, pitch_extractor, plbert)\n",
    "\n",
    "_ = [nets[key].eval() for key in nets]\n",
    "_ = [nets[key].to(device) for key in nets]\n",
    "\n",
    "params_whole = torch.load(\"Models/LibriTTS/epochs_2nd_00020.pth\", map_location='cpu')\n",
    "params = params_whole['net']\n",
    "\n",
    "for key in nets:\n",
    "    if key in params:\n",
    "        print('%s loaded' % key)\n",
    "        try:\n",
    "            nets[key].load_state_dict(params[key])\n",
    "        except:\n",
    "            from collections import OrderedDict\n",
    "            state_dict = params[key]\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                name = k[7:] # remove `module.`\n",
    "                new_state_dict[name] = v\n",
    "            # load params\n",
    "            nets[key].load_state_dict(new_state_dict, strict=False)\n",
    "_ = [nets[key].eval() for key in nets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d25ad0-9589-4b54-8f6d-6be86c13d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule\n",
    "\n",
    "# build diffucion samples\n",
    "sampler = DiffusionSampler(\n",
    "    nets[\"diffusion\"].diffusion,\n",
    "    sampler=ADPM2Sampler(),\n",
    "    sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0), # empirical parameters\n",
    "    clamp=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0531a4-49d8-4177-b240-e357979f43f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_to_mask(lengths):\n",
    "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
    "    mask = torch.gt(mask+1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, nets: dict, model_params: dict, sampler):\n",
    "        super().__init__()\n",
    "        self.model_params = model_params\n",
    "        self.text_encoder = nets[\"text_encoder\"]\n",
    "        self.bert = nets[\"bert\"]\n",
    "        self.bert_encoder = nets[\"bert_encoder\"]\n",
    "        self.style_encoder = nets[\"style_encoder\"]\n",
    "        self.predictor_encoder = nets[\"predictor_encoder\"]\n",
    "        self.predictor = nets[\"predictor\"]\n",
    "        self.decoder = nets[\"decoder\"]\n",
    "        self.sampler = sampler\n",
    "\n",
    "    def forward(self, tokens, style_ref):  # ref_tokens\n",
    "        device = tokens.device\n",
    "        alpha = 0.3\n",
    "        beta = 0.7\n",
    "        diffusion_steps = 5\n",
    "        embedding_scale = 1\n",
    "\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "        text_mask = length_to_mask(input_lengths).to(device)\n",
    "        # input_lengths = tokens.shape[-1]\n",
    "\n",
    "        t_en = self.text_encoder(tokens, input_lengths, text_mask)\n",
    "        bert_dur = self.bert(tokens, attention_mask=(~text_mask).int())\n",
    "        d_en = self.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "        # ref_input_lengths = torch.LongTensor([ref_tokens.shape[-1]]).to(device)\n",
    "        # ref_text_mask = length_to_mask(ref_input_lengths).to(device)\n",
    "        # ref_bert_dur = self.bert(ref_tokens, attention_mask=(~ref_text_mask).int())\n",
    "\n",
    "        s_pred = self.sampler(\n",
    "            noise = torch.randn((1, 256)).unsqueeze(1).to(device),\n",
    "            embedding=bert_dur,\n",
    "            embedding_scale=embedding_scale,\n",
    "            features=style_ref, # reference from the same speaker as the embedding\n",
    "            num_steps=diffusion_steps,\n",
    "        ).squeeze(1)\n",
    "        \n",
    "        s = s_pred[:, 128:]\n",
    "        ref = s_pred[:, :128]\n",
    "    \n",
    "        ref = alpha * ref + (1 - alpha)  * style_ref[:, :128]\n",
    "        s = beta * s + (1 - beta)  * style_ref[:, 128:]\n",
    "    \n",
    "        d = self.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
    "    \n",
    "        x, _ = self.predictor.lstm(d)\n",
    "        duration = self.predictor.duration_proj(x)\n",
    "    \n",
    "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1).to(torch.int64)\n",
    "    \n",
    "        pred_aln_trg = torch.zeros(input_lengths, pred_dur.sum())\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame:c_frame + pred_dur[i]] = 1\n",
    "            c_frame += pred_dur[i]\n",
    "    \n",
    "        # encode prosody\n",
    "        en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
    "        if self.model_params[\"decoder\"][\"type\"] == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(en)\n",
    "            asr_new[:, :, 0] = en[:, :, 0]\n",
    "            asr_new[:, :, 1:] = en[:, :, 0:-1]\n",
    "            en = asr_new\n",
    "\n",
    "        F0_pred, N_pred = self.predictor.F0Ntrain(en, s)\n",
    "    \n",
    "        asr = (t_en @ pred_aln_trg.unsqueeze(0).to(device))\n",
    "        if self.model_params[\"decoder\"][\"type\"] == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(asr)\n",
    "            asr_new[:, :, 0] = asr[:, :, 0]\n",
    "            asr_new[:, :, 1:] = asr[:, :, 0:-1]\n",
    "            asr = asr_new\n",
    "\n",
    "        out = self.decoder(asr, F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "        \n",
    "        return out.squeeze()[..., :-50] # weird pulse at the end of the model, need to be fixed later\n",
    "\n",
    "model = Model(nets, model_params, sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e19da-274b-44f7-93e5-857f4478eff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleModel(nn.Module):\n",
    "    def __init__(self, nets: dict):\n",
    "        super().__init__()\n",
    "        self.to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "            n_mels=80, n_fft=2048, win_length=1200, hop_length=300\n",
    "        )\n",
    "        self.style_encoder = nets[\"style_encoder\"]\n",
    "        self.predictor_encoder = nets[\"predictor_encoder\"]\n",
    "\n",
    "    def preprocess(self, wave):\n",
    "        mean, std = -4, 4\n",
    "        mel_tensor = self.to_mel(wave)\n",
    "        mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "        return mel_tensor\n",
    "\n",
    "    def forward(self, audio):\n",
    "        mel_tensor = self.preprocess(audio)\n",
    "        ref_s = self.style_encoder(mel_tensor.unsqueeze(1))\n",
    "        ref_p = self.predictor_encoder(mel_tensor.unsqueeze(1))\n",
    "\n",
    "        return torch.cat([ref_s, ref_p], dim=1)\n",
    "\n",
    "\n",
    "style_model = StyleModel(nets).to(device)\n",
    "\n",
    "\n",
    "def compute_style(path):\n",
    "    with torch.no_grad():\n",
    "        audio_tensor = torch.tensor(audio).to(device)\n",
    "        style_ref = style_model(audio_tensor)\n",
    "    return style_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac9de64-9f8d-46b8-9017-716ef30fb869",
   "metadata": {},
   "source": [
    "## Style Transfer Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2fff7f-9035-4198-9fa4-7bbeb116bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Demo/reference_audio/1221-135767-0014.wav\"\n",
    "style_ref = compute_style(path)\n",
    "\n",
    "# reference texts to sample styles\n",
    "# ref_texts = {}\n",
    "# ref_texts['Happy'] = \"We are happy to invite you to join us on a journey to the past, where we will visit the most amazing monuments ever built by human hands.\"\n",
    "# ref_texts['Sad'] = \"I am sorry to say that we have suffered a severe setback in our efforts to restore prosperity and confidence.\"\n",
    "# ref_texts['Angry'] = \"The field of astronomy is a joke! Its theories are based on flawed observations and biased interpretations!\"\n",
    "# ref_texts['Surprised'] = \"I can't believe it! You mean to tell me that you have discovered a new species of bacteria in this pond?\"\n",
    "\n",
    "text = \"Yea, his honourable worship is within, but he hath a godly minister or two with him, and likewise a leech.\"\n",
    "text = text.strip()\n",
    "ps = global_phonemizer.phonemize([text])\n",
    "ps = word_tokenize(ps[0])\n",
    "ps = ' '.join(ps)\n",
    "tokens = textclenaer(ps)\n",
    "tokens.insert(0, 0)\n",
    "tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    wav = model(tokens, style_ref).cpu().numpy()\n",
    "\n",
    "display(ipd.Audio(wav, rate=24000, normalize=False))\n",
    "\n",
    "# for style, ref_text in ref_texts.items():\n",
    "#     # wav = STinference(text, s_ref, v, diffusion_steps=10, alpha=0.5, beta=0.9, embedding_scale=1.5)\n",
    "\n",
    "#     ref_text = ref_text.strip()\n",
    "#     ps = global_phonemizer.phonemize([ref_text])\n",
    "#     ps = word_tokenize(ps[0])\n",
    "#     ps = ' '.join(ps)\n",
    "    \n",
    "#     ref_tokens = textclenaer(ps)\n",
    "#     ref_tokens.insert(0, 0)\n",
    "#     ref_tokens = torch.LongTensor(ref_tokens).to(device).unsqueeze(0)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         wav = model(tokens, ref_tokens, s_ref).cpu().numpy()\n",
    "    \n",
    "#     print(f\"{style}: \")\n",
    "#     display(ipd.Audio(wav, rate=24000, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a5376-86c8-424a-b30b-8d2929dfba2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0217a43c-f7c8-411f-a8e3-6bee8410379e",
   "metadata": {},
   "source": [
    "## ONNX Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45e439-9159-4ace-a34b-47e08e381961",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# for m in model.modules():\n",
    "#     # if 'instancenorm' in m.__class__.__name__.lower():\n",
    "#     m.train(False)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    args=(tokens, style_ref),  # ref_tokens\n",
    "    f=\"StyleTTS2.onnx\",\n",
    "    verbose=False,\n",
    "    opset_version=16,\n",
    "    training=torch.onnx.TrainingMode.EVAL,\n",
    "    input_names=[\"tokens\", \"style_ref\"],  # ref_tokens\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"tokens\": {1: \"seq_length\"},\n",
    "        # \"ref_tokens\": {1: \"seq_length\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096e388a-633e-4859-98ae-f20261f1ac77",
   "metadata": {},
   "source": [
    "## Test ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d5a3cd-8a73-4cf4-9034-d63d22afc70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"StyleTTS2.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession('StyleTTS2.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9003bb83-5336-486c-9cd9-72229930d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Demo/reference_audio/1221-135767-0014.wav\"\n",
    "style_ref = compute_style(path)\n",
    "\n",
    "text = \"Yea, his honourable worship is within, but he hath a godly minister or two with him, and likewise a leech.\"\n",
    "text = text.strip()\n",
    "ps = global_phonemizer.phonemize([text])\n",
    "ps = word_tokenize(ps[0])\n",
    "ps = ' '.join(ps)\n",
    "tokens = textclenaer(ps)\n",
    "tokens.insert(0, 0)\n",
    "tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "outputs = ort_session.run(None, {\n",
    "    'tokens': tokens.cpu().numpy(), \n",
    "    \"style_ref\": style_ref.cpu().numpy(),\n",
    "})\n",
    "wav = outputs[0]\n",
    "\n",
    "display(ipd.Audio(wav, rate=24000, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d436cdc-7421-43f5-bd3d-1c235fa92500",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Demo/reference_audio/1221-135767-0014.wav\"\n",
    "style_ref = compute_style(path)\n",
    "\n",
    "text = \"Hello hello, this is test. How are you doing?\"\n",
    "text = text.strip()\n",
    "ps = global_phonemizer.phonemize([text])\n",
    "ps = word_tokenize(ps[0])\n",
    "ps = ' '.join(ps)\n",
    "tokens = textclenaer(ps)\n",
    "tokens.insert(0, 0)\n",
    "tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "outputs = ort_session.run(None, {\n",
    "    'tokens': tokens.cpu().numpy(), \n",
    "    \"style_ref\": style_ref.cpu().numpy(),\n",
    "})\n",
    "wav = outputs[0]\n",
    "\n",
    "display(ipd.Audio(wav, rate=24000, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1bf229-ff71-4a12-80c7-f4385c3b831d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
